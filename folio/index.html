<!DOCTYPE html>
<html>
    
  <head>
    <meta charset="UTF-8">
    <title>i'm sebas</title>
    
    <link rel="stylesheet" href="index.css">
    <link rel="icon" type="image/png" href="img/ico.png">
    
    <script src="../lib/jquery-3.1.1.min.js" type="text/javascript"></script>
    <script src="main.js" type="text/javascript"></script>
  </head>
    
  <body>
      <div>
          <h2>
            <ul class="collapsible">
                <li class="dropdown"><a class="">MARIO AI</a>
                    <ul>
                        <li>
                            UCLA, CS 188: <i>AI Playing Games</i> (Spring 2016)<br>
                            <br>
                            <i>Super Mario</i> is a solved problem. Any number of autonomous
                            agents can play the game well. For us, <i>Super Mario</i> was a
                            platform from which we could ask questions not of performance,
                            but of logic: what are the rules of behavior that fix within the
                            “black box” of neural networks? and what methods can we use to
                            distill these rules?<br>
                            <br>
                            Our answer was a synthesis of distributed and sequential logic.<br>
                            <br>
                            We convened a data set of 100,000 snapshots of player and
                            environment state. This data was culled from the recorded gameplay
                            of a neural network agent and was fed into a self-organizing map.
                            The map clustered snapshots about a two-dimensional plane,
                            reducing input dimensionality. Labeled by hand, the resultant
                            clusters were taken to be symbolic of a set of prototypical game
                            states a player might encounter. They determined the input to our
                            state-action table.<br>
                            <br>
                            A state-action table expresses the expected value of an action in
                            response to a given state. As coded in our table, such expectations
                            were informed by reinforcement learning. Unlike orthodox
                            reinforcement learning, our conditioning was not by direct agency,
                            but by observation of a neural network agent. Our actions were
                            symbolic of the set of all coherent combinations of key presses on
                            the game controller.<br>
                            <br>
                            By its nature, a fixed state-action table may be reduced to a series
                            of conditional statements: “if state then action.”  We worked to
                            make our state-action table into an accurate simulation of the neural
                            network. If we were successful, we could by transitivity approximate the
                            “program” fixed within the neural network as a series of conditional
                            statements.<br>
                            <br>
                        </li>
                        <li><iframe width="660" height="370" src="https://www.youtube.com/embed/-ibO6uv-yVM?list=PLNfka63QcB7PdrU2V6s9hbyIabicqVusg" frameborder="0" allowfullscreen></iframe></li>
                        <li>
                            <br>
                            We found that we could not adequately simulate the behavior of a neural
                            network.<br>
                            <br>
                            The strategies which manifested in our agent were fairly crude: our
                            Mario leapt in the presence of Goombas, waited for Piranha Plants to
                            disappear from view. These tactics were aspects of the neural network’s
                            behavior, assuredly; however, they were oversimplifications. Our agent’s
                            shortcomings were due to a lack of state granularity and a naivety to
                            long-term rewards.<br>
                            <br>
                            By operating on state classes, our agent was desensitized to possible
                            interactions between features within a state class. The neural network
                            remained mindful of such interactions by operating on state features
                            directly. Our agent, as a result, could not identify exceptional states
                            or subclasses as known to the neural network: reinforcement learning
                            worked in futility toward the representation of multiple strategies
                            within the space of a single strategy. Discriminable states were poorly
                            convoluted within single state classes.<br>
                            <br>
                            Our agent’s behavior indicated no understanding of the long-term value
                            of progress. Loitering was a recurring tactic, as idleness guaranteed
                            Mario’s score and well-being. This tendency toward stability was an
                            impediment to the ultimate goal of stage completion. We attempted to solve this issue by the addition of a motion heuristic:
                            “always consider forward motion to be more valuable than idleness.” This
                            adjustment was only moderately useful. Our agent often protested by
                            driving itself into walls, standing still in effect.<br>
                            <br>
                            In pursuit of a more representative set of state classes, we recommend feature
                            selection and hierarchical clustering. In pursuit of goal-oriented
                            behavior, we recommend some prediction schema: such a mechanism would
                            determine a present action not only by its expected value, but also by
                            the expected value of subsequent actions in predicted future states.<br>
                            <br>                            
                            Our self-organizing map was sourced from the
                            <a href="http://java-ml.sourceforge.net/" target="_blank">Java-ML library</a>.
                            Our agent
                            was built upon the Java codebase provided by
                            <a href="http://www.marioai.org/" target="_blank">Mario AI Championship 2012</a>.
                            Our code is available on
                            <a href="https://github.com/MatteoVH/marioai" target="_blank">GitHub</a>.
                            An introductory presentation I have prepared on reinforcement learning
                            and neural networks can be found
                            <a href="img/mario/machine_learning_presentation_cs35.pdf" target="_blank">here</a>.
                        </li>
                    </ul>
                </li>
                <li class="dropdown"><a class="">GENRE CLUSTERING</a>
                    <ul>
                        <li><img src="img/genre/SOM_rgb_inputs.png"></li>
                        <li><img src="img/genre/SOM_rgb_reduction.png"></li>
                        <li><img src="img/genre/SOM_rgb_clusters.png"></li>
                        <li><img src="img/genre/SOM_test_4.png"></li>
                        <li><img src="img/genre/SOM_test_raw.png"></li>
                        <li><img src="img/genre/SOM_test_z.png"></li>
                    </ul>
                </li>
                <li class="dropdown"><a class="">MOTION OPPONENCY</a>
                    <ul>
                        <li>Summary</li>
                        <li><a href="img/motion/manuscript.pdf">manuscript</a></li>
                    </ul>
                </li>
                <li class="dropdown"><a class="">TRAVELING SALESMAN</a>
                    <ul>
                        <li>
                            I've recently become interested in the relationship
                            between computibility and topology. I've moved the Traveling
                            Salesman Problem to a flat torus (as opposed to bounded).
                            I have modeled the cities as interacting automata.
                            The inspiration is molecular bonding behavior.
                            Although I've not taken specific rules from bonding, I have
                            adopted the sentiment of electronegativity and stability: an
                            automata wants to link, and wants to link to its best matches.
                            If its not at a best match, it will break a poor match and link
                            to a better one. I wonder if this transformation of the problem
                            topology results in a transformation of the solution topology
                            that is parabolic: regardless of the starting conditions, the
                            state changes of the automata ultimately result in a halt
                            (due to "most stable" position being reached) at the ideal
                            solution.
                        </li>
                    </ul>
                </li>
                <li class="dropdown"><a class="">RUBIK'S CYCLE</a>
                    <ul>
                        <li><a href="#">Content</a></li>
                        <li><a href="#">Comments</a></li>
                        <li><a href="#">Tags</a></li>
                    </ul>
                </li>
                <li class="dropdown"><a class="">ANALOGIES</a>
                    <ul>
                        <li><a href="http://analogies.thatsebas.com/">download</a></li>
                        <li><a href="http://www.suspendmag.com/blog/sleepparalysis">SUSPEND feature</a></li>
                    </ul>
                </li>               
              </ul>
            </h2>

            <h1>
                <a href="../">return</a><br>
            </h1>
       </div>
  </body>
    
</html>